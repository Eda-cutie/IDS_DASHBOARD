# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i1C7_bWn96yJi0kvLywZqG04igR8CNqP
"""

import os, json, joblib
import numpy as np
import pandas as pd
import streamlit as st
import plotly.express as px
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest, RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, RocCurveDisplay

st.set_page_config(page_title="CICIDS IDS Dashboard", layout="wide")

# --- Keywords for DoS/DDoS ---
ATTACK_KEYWORDS = ["ddos", "dos", "hulk", "goldeneye", "slowloris", "slowhttptest"]

def infer_binary_label(label: str) -> int:
    s = str(label).lower()
    if s == "benign":
        return 0
    return int(any(k in s for k in ATTACK_KEYWORDS))

def plot_confusion_matrix(y_true, y_pred):
    cm = confusion_matrix(y_true, y_pred, labels=[0,1])
    fig, ax = plt.subplots(figsize=(4,4))
    ax.imshow(cm, interpolation='nearest')
    ax.set_title('Confusion Matrix')
    ax.set_xticks([0,1]); ax.set_xticklabels(['Benign','Attack'])
    ax.set_yticks([0,1]); ax.set_yticklabels(['Benign','Attack'])
    for (i,j), z in np.ndenumerate(cm):
        ax.text(j, i, f"{z}", ha='center', va='center')
    ax.set_xlabel('Predicted'); ax.set_ylabel('True')
    fig.tight_layout()
    return fig

def plot_roc(y_true, scores, label_name):
    try:
        auc = roc_auc_score(y_true, scores)
    except Exception:
        auc = float('nan')
    fig, ax = plt.subplots(figsize=(4,4))
    RocCurveDisplay.from_predictions(y_true, scores, ax=ax, name=f"{label_name} AUC={auc:.3f}")
    ax.set_title(f'ROC Curve ‚Äî {label_name}')
    fig.tight_layout()
    return fig, auc

def plot_hist(scores):
    fig, ax = plt.subplots(figsize=(5,3))
    ax.hist(scores, bins=50)
    ax.set_title('Anomaly Score Distribution')
    ax.set_xlabel('Score (higher=worse)')
    ax.set_ylabel('Count')
    fig.tight_layout()
    return fig

def train_and_evaluate(df, test_size=0.2, random_state=42):
    if "Label" not in df.columns:
        raise ValueError("CSV must contain a 'Label' column.")

    y = df["Label"].apply(infer_binary_label).astype(int)
    X = df.select_dtypes(include=[np.number]).copy()
    if X.empty:
        raise ValueError("No numeric feature columns found.")

    X_train, X_test, y_train, y_test, raw_train, raw_test = train_test_split(
        X, y, df, test_size=test_size, random_state=random_state, stratify=y
    )

    scaler = StandardScaler()
    scaler.fit(X_train)
    X_train_s = scaler.transform(X_train)
    X_test_s = scaler.transform(X_test)

    results = {}

    # --- IsolationForest ---
    benign_mask = (y_train == 0)
    iforest = IsolationForest(
        n_estimators=300,
        max_samples='auto',
        contamination='auto',
        random_state=random_state,
        n_jobs=-1
    )
    iforest.fit(X_train_s[benign_mask])
    decision = iforest.decision_function(X_test_s)
    anomaly_score = (-decision)
    pred_iso = (iforest.predict(X_test_s) == -1).astype(int)

    report_iso = classification_report(y_test, pred_iso, target_names=["Benign","Attack"], output_dict=True, zero_division=0)
    roc_auc_iso = roc_auc_score(y_test, anomaly_score)

    iso_df = raw_test.copy()
    iso_df["y_true"] = y_test.values
    iso_df["anomaly_score"] = anomaly_score
    iso_df["y_pred"] = pred_iso

    results["iso"] = {
        "report": report_iso,
        "roc_auc": roc_auc_iso,
        "df": iso_df,
        "cm": plot_confusion_matrix(y_test, pred_iso),
        "roc": plot_roc(y_test, anomaly_score, "IsolationForest")[0],
        "hist": plot_hist(anomaly_score)
    }

    # --- RandomForest ---
    rf = RandomForestClassifier(n_estimators=300, random_state=random_state, n_jobs=-1)
    rf.fit(X_train_s, y_train)
    proba_rf = rf.predict_proba(X_test_s)[:, 1]
    pred_rf = (proba_rf >= 0.5).astype(int)

    report_rf = classification_report(y_test, pred_rf, target_names=["Benign","Attack"], output_dict=True, zero_division=0)
    roc_auc_rf = roc_auc_score(y_test, proba_rf)

    rf_df = raw_test.copy()
    rf_df["y_true"] = y_test.values
    rf_df["attack_probability"] = proba_rf
    rf_df["y_pred"] = pred_rf

    results["rf"] = {
        "report": report_rf,
        "roc_auc": roc_auc_rf,
        "df": rf_df,
        "cm": plot_confusion_matrix(y_test, pred_rf),
        "roc": plot_roc(y_test, proba_rf, "RandomForest")[0],
        "hist": None
    }

    return results

# --- Streamlit UI ---
st.title("üîç CICIDS2017 IDS Dashboard ‚Äî DoS/DDoS Detection")
st.caption("Train & compare IsolationForest (unsupervised) vs RandomForest (supervised)")

uploaded_file = st.file_uploader("Upload CICIDS2017 CSV file (cleaned)", type=["csv"])

if uploaded_file:
    df = pd.read_csv(uploaded_file)

    with st.spinner("Training models... this may take a while ‚è≥"):
        results = train_and_evaluate(df)

    model_choice = st.sidebar.radio(
        "Select Model to View",
        options=["IsolationForest", "RandomForest"],
        index=0
    )

    model_key = "iso" if model_choice == "IsolationForest" else "rf"
    res = results[model_key]

    # ==== Metrics ====
    roc_auc = res["roc_auc"]
    report = res["report"]

    col1, col2, col3, col4 = st.columns(4)
    col1.metric("ROC-AUC", f"{roc_auc:.3f}")
    attk_f1 = report.get("Attack", {}).get("f1-score", float("nan"))
    ben_f1 = report.get("Benign", {}).get("f1-score", float("nan"))
    col2.metric("Attack F1", f"{attk_f1:.3f}")
    col3.metric("Benign F1", f"{ben_f1:.3f}")
    col4.metric("Samples (test)", f"{len(res['df']):,}")

    st.markdown("---")
    c1, c2, c3 = st.columns([1,1,1])
    with c1:
        st.subheader("Confusion Matrix")
        st.pyplot(res["cm"])
    with c2:
        st.subheader("ROC Curve")
        st.pyplot(res["roc"])
    if model_key == "iso" and res["hist"] is not None:
        with c3:
            st.subheader("Anomaly Score Histogram")
            st.pyplot(res["hist"])

    # ==== Predictions Table ====
    st.markdown("---")
    st.subheader("Interactive Predictions Table")
    df_view = res["df"]
    label_options = sorted(df_view["Label"].astype(str).unique().tolist()) if "Label" in df_view.columns else []
    label_filter = st.multiselect("Filter by original Label", label_options, default=label_options)
    if label_options:
        df_view = df_view[df_view["Label"].astype(str).isin(label_filter)] if label_filter else df_view

    st.write(f"Showing **{len(df_view):,}** rows")
    st.dataframe(df_view.head(2000), use_container_width=True, height=400)

    st.markdown("---")
    st.subheader("Attack vs Benign ‚Äî Predicted")
    if "y_pred" in df_view.columns:
        pie = px.pie(df_view, names=df_view["y_pred"].map({0:"Benign", 1:"Attack"}), title="Predicted class breakdown")
        st.plotly_chart(pie, use_container_width=True)
else:
    st.info("üëÜ Please upload a CICIDS2017 CSV file to start.")