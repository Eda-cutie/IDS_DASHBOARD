# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VtBblDxy1xceoepXTUZVzZGNeSmpZLZS
"""

import os
import joblib
import numpy as np
import pandas as pd
import streamlit as st
import plotly.express as px
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest, RandomForestClassifier
from sklearn.metrics import (
    classification_report,
    roc_auc_score,
    confusion_matrix,
    roc_curve,
    f1_score
)

st.set_page_config(page_title="CICIDS2017 IDS Dashboard", layout="wide")

# ============== Sidebar ==============
st.sidebar.title("‚öôÔ∏è IDS Controls")
uploaded_file = st.sidebar.file_uploader("Upload CSV file", type=["csv"])

model_choice = st.sidebar.radio(
    "Select Model",
    ["Random Forest", "Isolation Forest", "Compare Both"]
)

# ============== Load Data ==============
if uploaded_file:
    df = pd.read_csv(uploaded_file)
    st.success(f"‚úÖ Loaded dataset with {df.shape[0]:,} rows and {df.shape[1]:,} columns")

    # Features/target (assuming 'Label' column exists)
    X = df.drop(columns=["Label"], errors="ignore")
    y = df["Label"] if "Label" in df.columns else None

    # Train/test split
    if y is not None:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42, stratify=y
        )
    else:
        X_train, X_test = train_test_split(X, test_size=0.3, random_state=42)
        y_train = y_test = None

    # Scale
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    results = {}

    # Random Forest
    if model_choice in ["Random Forest", "Compare Both"]:
        rf = RandomForestClassifier(n_estimators=100, random_state=42)
        rf.fit(X_train_scaled, y_train)
        y_pred = rf.predict(X_test_scaled)
        y_prob = rf.predict_proba(X_test_scaled)[:, 1]

        auc = roc_auc_score(y_test, y_prob)
        f1_attack = f1_score(y_test, y_pred, pos_label="Attack") if "Attack" in np.unique(y_test) else None
        f1_benign = f1_score(y_test, y_pred, pos_label="Benign") if "Benign" in np.unique(y_test) else None

        results["Random Forest"] = {
            "model": rf,
            "y_pred": y_pred,
            "y_prob": y_prob,
            "auc": auc,
            "f1_attack": f1_attack,
            "f1_benign": f1_benign,
            "df": X_test.assign(Label=y_test.values, y_pred=y_pred, Score=y_prob)
        }

    # Isolation Forest
    if model_choice in ["Isolation Forest", "Compare Both"]:
        iso = IsolationForest(contamination=0.05, random_state=42)
        iso.fit(X_train_scaled)
        y_pred_iso = iso.predict(X_test_scaled)
        y_pred_iso = np.where(y_pred_iso == -1, 1, 0)  # -1 anomaly ‚Üí 1 Attack
        y_prob_iso = -iso.decision_function(X_test_scaled)

        auc_iso = roc_auc_score(y_test.map({"Benign": 0, "Attack": 1}), y_prob_iso) if y_test is not None else None
        f1_attack_iso = f1_score(y_test.map({"Benign": 0, "Attack": 1}), y_pred_iso, pos_label=1) if y_test is not None else None
        f1_benign_iso = f1_score(y_test.map({"Benign": 0, "Attack": 1}), y_pred_iso, pos_label=0) if y_test is not None else None

        results["Isolation Forest"] = {
            "model": iso,
            "y_pred": y_pred_iso,
            "y_prob": y_prob_iso,
            "auc": auc_iso,
            "f1_attack": f1_attack_iso,
            "f1_benign": f1_benign_iso,
            "df": X_test.assign(Label=y_test.values, y_pred=y_pred_iso, Score=y_prob_iso)
        }

    # ============== Dashboard Layout ==============
    for mode, res in results.items():
        st.header(f"üìä {mode} Results")

        # ==== Top Metrics ====
        c1, c2, c3, c4 = st.columns(4)
        c1.metric("ROC-AUC", f"{res['auc']:.3f}" if res['auc'] else "N/A")
        c2.metric("F1 (Attack)", f"{res['f1_attack']:.3f}" if res['f1_attack'] else "N/A")
        c3.metric("F1 (Benign)", f"{res['f1_benign']:.3f}" if res['f1_benign'] else "N/A")
        c4.metric("Samples", len(res["df"]))

        # ==== Confusion Matrix + ROC Curve + Histogram ====
        c5, c6, c7 = st.columns(3)

        with c5:
            st.subheader("Confusion Matrix")
            if y_test is not None:
                cm = confusion_matrix(y_test, res["y_pred"], labels=np.unique(y_test))
                fig = px.imshow(cm, text_auto=True, x=np.unique(y_test), y=np.unique(y_test),
                                labels=dict(x="Predicted", y="Actual", color="Count"),
                                title="Confusion Matrix")
                st.plotly_chart(fig, use_container_width=True)

        with c6:
            st.subheader("ROC Curve")
            if y_test is not None and res["y_prob"] is not None:
                fpr, tpr, _ = roc_curve(y_test.map({"Benign": 0, "Attack": 1}), res["y_prob"])
                fig, ax = plt.subplots()
                ax.plot(fpr, tpr, label=f"{mode} (AUC={res['auc']:.2f})")
                ax.plot([0, 1], [0, 1], "k--")
                ax.set_xlabel("False Positive Rate")
                ax.set_ylabel("True Positive Rate")
                ax.legend()
                st.pyplot(fig)

        with c7:
            st.subheader("Anomaly Score Histogram")
            fig = px.histogram(res["df"], x="Score", nbins=50, title="Score Distribution")
            st.plotly_chart(fig, use_container_width=True)

        # ==== Predictions Table + Pie chart + Download ====
        st.subheader("Interactive Predictions Table")
        df_view = res["df"]

        # --- Label filter ---
        label_options = sorted(df_view["Label"].astype(str).unique().tolist()) if "Label" in df_view.columns else []
        label_filter = st.multiselect("Filter by original Label", label_options, default=label_options)
        if label_options:
            df_view = df_view[df_view["Label"].astype(str).isin(label_filter)] if label_filter else df_view

        # --- Column selection ---
        all_columns = df_view.columns.tolist()
        selected_columns = st.multiselect(
            "Select columns to display",
            options=all_columns,
            default=all_columns
        )
        df_display = df_view[selected_columns] if selected_columns else df_view

        # --- Table + Download ---
        c8, c9 = st.columns([2,1])
        with c8:
            st.write(f"Showing **{len(df_display):,}** rows")
            st.dataframe(df_display, use_container_width=True, height=400)

            # CSV now respects the selected columns
            csv = df_display.to_csv(index=False).encode("utf-8")
            st.download_button(
                "üì• Download Predictions CSV",
                csv,
                file_name=f"{mode}_predictions.csv",
                mime="text/csv"
            )

        with c9:
            st.subheader("Attack vs Benign ‚Äî Predicted")
            if "y_pred" in df_view.columns:
                pie = px.pie(
                    df_view,
                    names=df_view["y_pred"].map({0: "Benign", 1: "Attack"}),
                    title="Predicted class breakdown"
                )
                st.plotly_chart(pie, use_container_width=True)

else:
    st.warning("üìÇ Please upload a CICIDS2017 CSV file to continue.")