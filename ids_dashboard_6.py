# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i1C7_bWn96yJi0kvLywZqG04igR8CNqP
"""

import os, json, joblib
import numpy as np
import pandas as pd
import streamlit as st
import plotly.express as px
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest, RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, RocCurveDisplay

st.set_page_config(page_title="CICIDS IDS Dashboard", layout="wide")

# --- Keywords for DoS/DDoS ---
ATTACK_KEYWORDS = ["ddos", "dos", "hulk", "goldeneye", "slowloris", "slowhttptest"]

def infer_binary_label(label: str) -> int:
    s = str(label).lower()
    if s == "benign":
        return 0
    return int(any(k in s for k in ATTACK_KEYWORDS))

def plot_confusion_matrix(y_true, y_pred):
    cm = confusion_matrix(y_true, y_pred, labels=[0,1])
    fig, ax = plt.subplots(figsize=(4,4))
    ax.imshow(cm, interpolation='nearest')
    ax.set_title('Confusion Matrix')
    ax.set_xticks([0,1]); ax.set_xticklabels(['Benign','Attack'])
    ax.set_yticks([0,1]); ax.set_yticklabels(['Benign','Attack'])
    for (i,j), z in np.ndenumerate(cm):
        ax.text(j, i, f"{z}", ha='center', va='center')
    ax.set_xlabel('Predicted'); ax.set_ylabel('True')
    fig.tight_layout()
    return fig

def plot_roc(y_true, scores, label_name):
    try:
        auc = roc_auc_score(y_true, scores)
    except Exception:
        auc = float('nan')
    fig, ax = plt.subplots(figsize=(4,4))
    RocCurveDisplay.from_predictions(y_true, scores, ax=ax, name=f"{label_name} AUC={auc:.3f}")
    ax.set_title(f'ROC Curve ‚Äî {label_name}')
    fig.tight_layout()
    return fig, auc

def plot_hist(scores):
    fig, ax = plt.subplots(figsize=(5,3))
    ax.hist(scores, bins=50)
    ax.set_title('Anomaly Score Distribution')
    ax.set_xlabel('Score (higher=worse)')
    ax.set_ylabel('Count')
    fig.tight_layout()
    return fig

def train_and_evaluate(df, test_size=0.2, random_state=42):
    if "Label" not in df.columns:
        raise ValueError("CSV must contain a 'Label' column.")

    y = df["Label"].apply(infer_binary_label).astype(int)
    X = df.select_dtypes(include=[np.number]).copy()
    if X.empty:
        raise ValueError("No numeric feature columns found.")

    X_train, X_test, y_train, y_test, raw_train, raw_test = train_test_split(
        X, y, df, test_size=test_size, random_state=random_state, stratify=y
    )

    scaler = StandardScaler()
    scaler.fit(X_train)
    X_train_s = scaler.transform(X_train)
    X_test_s = scaler.transform(X_test)

    results = {}

    # --- IsolationForest ---
    benign_mask = (y_train == 0)
    iforest = IsolationForest(
        n_estimators=300,
        max_samples='auto',
        contamination='auto',
        random_state=random_state,
        n_jobs=-1
    )
    iforest.fit(X_train_s[benign_mask])
    decision = iforest.decision_function(X_test_s)
    anomaly_score = (-decision)
    pred_iso = (iforest.predict(X_test_s) == -1).astype(int)

    report_iso = classification_report(y_test, pred_iso, target_names=["Benign","Attack"], output_dict=True, zero_division=0)
    roc_auc_iso = roc_auc_score(y_test, anomaly_score)

    iso_df = raw_test.copy()
    iso_df["y_true"] = y_test.values
    iso_df["anomaly_score"] = anomaly_score
    iso_df["y_pred"] = pred_iso

    results["iso"] = {
        "report": report_iso,
        "roc_auc": roc_auc_iso,
        "df": iso_df,
        "cm": plot_confusion_matrix(y_test, pred_iso),
        "roc": plot_roc(y_test, anomaly_score, "IsolationForest")[0],
        "hist": plot_hist(anomaly_score)
    }

    # --- RandomForest ---
    rf = RandomForestClassifier(n_estimators=300, random_state=random_state, n_jobs=-1)
    rf.fit(X_train_s, y_train)
    proba_rf = rf.predict_proba(X_test_s)[:, 1]
    pred_rf = (proba_rf >= 0.5).astype(int)

    report_rf = classification_report(y_test, pred_rf, target_names=["Benign","Attack"], output_dict=True, zero_division=0)
    roc_auc_rf = roc_auc_score(y_test, proba_rf)

    rf_df = raw_test.copy()
    rf_df["y_true"] = y_test.values
    rf_df["attack_probability"] = proba_rf
    rf_df["y_pred"] = pred_rf

    results["rf"] = {
        "report": report_rf,
        "roc_auc": roc_auc_rf,
        "df": rf_df,
        "cm": plot_confusion_matrix(y_test, pred_rf),
        "roc": plot_roc(y_test, proba_rf, "RandomForest")[0],
        "hist": None
    }

    return results

# --- Streamlit UI ---
import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier, IsolationForest
from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score
from io import BytesIO

st.set_page_config(page_title="IDS Dashboard", layout="wide")

st.title("üöÄ Intrusion Detection System (IDS) Dashboard")
st.write("Upload the cleaned **CICIDS2017 dataset** to detect anomalies and compare models.")

# File uploader
uploaded_file = st.file_uploader("üìÇ Upload CICIDS2017 CSV file (cleaned)", type=["csv"])

# Mode selection
mode = st.selectbox("üîç Select Mode", ["Random Forest", "Isolation Forest", "Compare Both"])

if uploaded_file:
    df = pd.read_csv(uploaded_file)

    st.subheader("üîé Dataset Preview")
    st.dataframe(df.head())

    # Assume last column is target
    X = df.iloc[:, :-1]
    y = df.iloc[:, -1]

    # ---------------- Random Forest ----------------
    if mode in ["Random Forest", "Compare Both"]:
        rf = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)
        rf.fit(X, y)
        rf_preds = rf.predict(X)
        rf_acc = accuracy_score(y, rf_preds)
        rf_auc = roc_auc_score(y, rf.predict_proba(X)[:,1]) if len(np.unique(y)) == 2 else None

        st.subheader("üå≤ Random Forest Results")
        st.write(f"‚úÖ Accuracy: **{rf_acc:.4f}**")
        if rf_auc:
            st.write(f"‚úÖ ROC-AUC: **{rf_auc:.4f}**")

        # ROC Curve
        if len(np.unique(y)) == 2:
            fpr, tpr, _ = roc_curve(y, rf.predict_proba(X)[:,1])
            plt.figure()
            plt.plot(fpr, tpr, label=f"Random Forest (AUC={rf_auc:.2f})")
            plt.plot([0,1],[0,1],'--',color='gray')
            plt.xlabel("False Positive Rate")
            plt.ylabel("True Positive Rate")
            plt.title("ROC Curve (Random Forest)")
            plt.legend()
            st.pyplot(plt)

    # ---------------- Isolation Forest ----------------
    if mode in ["Isolation Forest", "Compare Both"]:
        iso = IsolationForest(contamination=0.1, random_state=42)
        iso.fit(X)
        iso_preds = iso.predict(X)
        iso_preds = np.where(iso_preds == -1, 1, 0)  # -1=anomaly ‚Üí 1
        iso_acc = accuracy_score(y, iso_preds)
        iso_auc = roc_auc_score(y, iso_preds) if len(np.unique(y)) == 2 else None

        st.subheader("‚ùÑÔ∏è Isolation Forest Results")
        st.write(f"‚úÖ Accuracy: **{iso_acc:.4f}**")
        if iso_auc:
            st.write(f"‚úÖ ROC-AUC: **{iso_auc:.4f}**")

    # ---------------- Comparison ----------------
    if mode == "Compare Both":
        st.subheader("üìä Model Comparison")
        metrics_df = pd.DataFrame({
            "Model": ["Random Forest", "Isolation Forest"],
            "Accuracy": [rf_acc, iso_acc],
            "ROC-AUC": [rf_auc, iso_auc]
        })
        st.table(metrics_df)

        # üìâ Bar chart comparison
        fig, ax = plt.subplots(figsize=(6,4))
        bar_width = 0.35
        models = metrics_df["Model"]
        accuracy_scores = metrics_df["Accuracy"]
        auc_scores = metrics_df["ROC-AUC"]

        index = np.arange(len(models))
        ax.bar(index, accuracy_scores, bar_width, label="Accuracy")
        ax.bar(index + bar_width, auc_scores, bar_width, label="ROC-AUC")

        ax.set_xlabel("Models")
        ax.set_ylabel("Score")
        ax.set_title("Model Performance Comparison")
        ax.set_xticks(index + bar_width / 2)
        ax.set_xticklabels(models)
        ax.legend()

        st.pyplot(fig)

    # ---------------- Predictions & Download ----------------
    st.subheader("üìÇ Predictions Preview")
    preds_df = df.copy()
    if mode in ["Random Forest", "Compare Both"]:
        preds_df["RF_Prediction"] = rf_preds
    if mode in ["Isolation Forest", "Compare Both"]:
        preds_df["IF_Prediction"] = iso_preds

    st.dataframe(preds_df.head())

    # Download predictions
    csv_buffer = BytesIO()
    preds_df.to_csv(csv_buffer, index=False)
    csv_buffer.seek(0)

    st.download_button(
        label="üì• Download Predictions CSV",
        data=csv_buffer,
        file_name="ids_predictions.csv",
        mime="text/csv"
    )